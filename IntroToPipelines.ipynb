{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1648e673-a3a2-4306-a5ce-ddde620e7ea9",
   "metadata": {},
   "source": [
    "## Introduction to Hugging Face and Pipelines\n",
    "\n",
    "**Hugging Face** is a company that has created a platform for building, training, and deploying machine learning models, especially those based on the Transformer architecture. One of the most popular aspects of Hugging Face is their Transformers library, which provides pre-trained models for a variety of tasks. \n",
    "\n",
    "**Pipelines** are a high-level component of the Transformers library. They abstract away most of the complicated details and provide a simple interface to use these models for various tasks such as text classification, translation, summarization, and more.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a026f683-7f6e-4b6f-b8ce-032fb4ddbcaf",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "Before we dive in, we need to set up our environment. This requires installing the Hugging Face Transformers library as well as other dependencies to make it work with paperspace. Normally, you would execute the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba7d13e-a27e-4aaf-b99c-827712c76725",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade transformers torch torchvision torchaudio\n",
    "!pip install -q tokenizers==0.14 evaluate\n",
    "!pip install -q bitsandbytes transformers accelerate gradio thread6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa2d55-f9ad-434a-a164-8fc53b5f4440",
   "metadata": {},
   "source": [
    "---\n",
    "## Overview of Different Pipelines\n",
    "\n",
    "Pipelines can be used for a multitude of tasks. Some of the common ones include:\n",
    "\n",
    "- **Text Classification**: Categorizing a piece of text into one or multiple categories.\n",
    "- **Token Classification**: Classifying individual words or tokens in a sentence.\n",
    "- **Text Generation**: Generating coherent and contextually relevant text based on a given prompt.\n",
    "- **Translation**: Translating text from one language to another.\n",
    "- **Summarization**: Providing a concise summary of a longer text.\n",
    "- ... and many more!\n",
    "\n",
    "For our demonstration, we'll focus on a single task \"Zero-Shot Image Classification\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489929f3-5de4-40db-a99a-2299daf89123",
   "metadata": {},
   "source": [
    "---\n",
    "## How does Zero-Shot Image Classification work?\n",
    "\n",
    "At a high level, zero-shot classification combines the power of natural language processing and vision. The idea is to represent both the image and possible labels (categories) in a shared embedding space. If the embeddings of the image and a label are close in this space, it means the image likely belongs to that category.\n",
    "\n",
    "**Here's a simplified step-by-step process:**\n",
    "   - The image is passed through a vision model (like a CNN) to get an image embedding.\n",
    "   - Each potential label (like \"cat\" or \"dog\") is passed through a text model to get label embeddings.\n",
    "   - The distance between the image embedding and each label embedding is computed.\n",
    "   - The label with the smallest distance to the image embedding is predicted as the category of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656de645-f050-4589-a9f2-d0febce95da4",
   "metadata": {},
   "source": [
    "---\n",
    "### Importing Libraries and Model Initialization\n",
    "\n",
    "In the realm of data analysis and machine learning, there are a multitude of tools and libraries at our disposal. Think of them as the various departments in a company, each specializing in a specific task. Just as a business would call upon its marketing department to launch a campaign, we call upon specific libraries to execute certain functions.\n",
    "\n",
    "In this cell, we're:\n",
    "1. **Importing Libraries:** Bringing in specialized tools to handle tasks like fetching images from the internet and processing them.\n",
    "2. **Model Initialization:** Setting up our machine learning model, which in this context, is like hiring an expert. We're using a model specifically designed for vision tasks, making it our 'visual expert' for the images we'll analyze.\n",
    "\n",
    "This setup is crucial as it lays the foundation for the tasks we'll execute in the subsequent steps. In business terms, it's akin to ensuring you have the right team \n",
    "and strategy before launching a project.\n",
    "\n",
    "\n",
    " **Disregard any warnings that may appear.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c6312-d212-475b-9956-72cca8a15610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests: To make HTTP requests and fetch the image from the given URL\n",
    "# transformers: Hugging Face's library for using transformer models\n",
    "# Image from PIL: To process and manipulate the image data\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "# Specify the checkpoint for the model we want to use.\n",
    "# In this case, we're using the \"openai/clip-vit-large-patch14\" which is a model designed for vision tasks.\n",
    "checkpoint = \"openai/clip-vit-large-patch14\"\n",
    "\n",
    "# Initialize the zero-shot image classification pipeline.\n",
    "# This pipeline will use the specified model checkpoint and is designed for zero-shot image classification.\n",
    "detector = pipeline(model=checkpoint, task=\"zero-shot-image-classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b895328-594a-46cc-ad18-de863e6efeca",
   "metadata": {},
   "source": [
    "### Fetching and Displaying the Image\n",
    "\n",
    "In this step, we are working with real-world data. We fetch an image directly from a provided URL. Imagine you're browsing the internet and you come across an image you'd like to categorize. Instead of manually downloading the image, we can programmatically fetch it using a simple line of code.\n",
    "\n",
    "The displayed image below is what we'll be attempting to classify in the next step. It's essential to visually inspect our data (in this case, the image) to have an understanding of what we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431b107f-66fe-4813-aafb-6b498ee18150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the URL of the image we want to classify.\n",
    "# This is an image from google and we'll fetch it directly using the requests library. (look at the lil doge)\n",
    "url = \"https://encrypted-tbn2.gstatic.com/images?q=tbn:ANd9GcR2v8jGQFEHwDE0bEIm2Sofs-0n5RUWyiNtY_JQw46IozVB-YPU\"\n",
    "\n",
    "# Fetch the image from the provided URL and open it using PIL's Image module.\n",
    "# The \"stream=True\" ensures that the content isn't downloaded until accessed, and \".raw\" provides the raw content.\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Display the fetched image (this will render the image in the Jupyter notebook or the respective environment).\n",
    "print(\"lil doge\")\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d5bc89-4fb3-42fb-9fcf-ff5bd1743198",
   "metadata": {},
   "source": [
    "### Image Classification\n",
    "\n",
    "Once we have our image, the next logical step is to determine what's depicted in it. Remember, in the world of business, quick and accurate decision-making can be crucial. If we had thousands of images, manually categorizing them would be time-consuming and prone to errors.\n",
    "\n",
    "Here, we use a powerful machine learning model for a task called \"zero-shot image classification.\" In layman's terms, this means that even if the model hasn't explicitly been trained on a specific category (e.g., \"fox\" or \"bear\"), it can still make educated guesses based on its knowledge. \n",
    "\n",
    "The results below show the model's confidence level for each category. The higher the score, the more confident the model is that the image belongs to that category. This is similar to how a business decision might be backed by data-driven insights, providing a level of confidence in the choices made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038e5071-5d4e-4bb7-be2f-2c5ae62e39a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the detector (zero-shot image classification pipeline) to classify the image.\n",
    "# We provide a list of candidate labels [\"fox\", \"bear\", \"seagull\", \"owl\"] and ask the model to classify the image among these labels.\n",
    "predictions = detector(image, candidate_labels=[\"fox\", \"bear\", \"seagull\", \"owl\"])\n",
    "\n",
    "# Print the classification results.\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a32bb0d-9204-4e65-8649-add115d04eaa",
   "metadata": {},
   "source": [
    "### Assignment: Image Classification Challenge\n",
    "\n",
    "Now that you've seen the power of machine learning in image classification, it's time to put your knowledge to the test!\n",
    "\n",
    "**Task:** \n",
    "1. Choose an image from the internet that you're curious about. It could be any subject or object, but preferably something not overly complex.\n",
    "2. Use the tools and code structure we've discussed to fetch the image and display it.\n",
    "3. Utilize the zero-shot image classification pipeline to classify your chosen image among a set of candidate labels you provide.\n",
    "4. Analyze the results. Did the model correctly classify your image? Were you surprised by the outcome?\n",
    "\n",
    "Remember, this is a hands-on way to understand the potential and limitations of machine learning models. By choosing different images and labels, you'll gain insights into how the model thinks and makes decisions.\n",
    "\n",
    "**Hint:** You can re-use and modify the code from the previous cells. Just make sure to provide a new image URL and set of candidate labels. Be creative and have fun!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916846e-5750-46ec-9703-f098d51fb848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Provide the URL of the image you want to classify\n",
    "url_assignment = \"YOUR_IMAGE_URL_HERE\"\n",
    "\n",
    "# TODO: Fetch the image from the provided URL and display it\n",
    "image_assignment = Image.open(requests.get(url_assignment, stream=True).raw)\n",
    "image_assignment\n",
    "\n",
    "# TODO: Classify the image using the zero-shot image classification pipeline\n",
    "# Provide a list of candidate labels for classification\n",
    "predictions_assignment = detector(image_assignment, candidate_labels=[\"YOUR_LABEL_1\", \"YOUR_LABEL_2\", \"YOUR_LABEL_3\", \"...\"])\n",
    "\n",
    "# Print the classification results\n",
    "print(predictions_assignment)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
