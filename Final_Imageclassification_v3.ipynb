{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b899355",
   "metadata": {},
   "source": [
    "\n",
    "# Image Classification with Huggingface's Deep Learning Model\n",
    "\n",
    "Image classification is the task of assigning a label to an image from a predefined set of categories. It's one of the core tasks in Computer Vision that, despite its simplicity, has a large variety of practical applications. \n",
    "\n",
    "In this lab, we'll use a model from Huggingface's model hub to classify images. Deep learning models, especially Convolutional Neural Networks (CNNs), have shown impressive performance on image classification tasks. They automatically and adaptively learn spatial hierarchies of features from images.\n",
    "___\n",
    "## How does Image Classification work?\n",
    "\n",
    "1. **Feature Extraction**: The model reads the image and extracts important features from it. This is done using convolutional layers, which have filters that slide over the image to capture local patterns.\n",
    "2. **Classification**: After extracting features, the model uses fully connected layers to classify the image into one of the categories.\n",
    "3. **Output**: The model outputs a probability distribution over all categories. The category with the highest probability is the model's prediction.\n",
    "\n",
    "Remember, the deeper the network (more layers), the more complex patterns it can capture. But it also becomes computationally expensive.\n",
    "\n",
    "Now, let's dive into the code and see the model in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdef5559",
   "metadata": {},
   "source": [
    "---\n",
    "## Understanding Image Classification at a Low Level\n",
    "\n",
    "The image below provides a visual representation of a Convolutional Neural Network (CNN), which is commonly used in image classification tasks.\n",
    "\n",
    "![CNN Architecture](https://www.mdpi.com/applsci/applsci-12-12873/article_deploy/html/images/applsci-12-12873-g001.png)\n",
    "\n",
    "---\n",
    "A CNN processes an image through multiple layers:\n",
    "- **Input Layer**: Takes the raw image pixels.\n",
    "- **Hidden Layers**: Layers in between the input and output, responsible for feature extraction and transformation.\n",
    "- **Output Layers**: The final layer that provides the classification result based on the processed features.\n",
    "\n",
    "Each layer transforms the image data, capturing patterns and features that help the model make its final prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25912b61-0cb9-4791-a25b-6647e5f04079",
   "metadata": {},
   "source": [
    "---\n",
    "# The model we'll be using in this notebook is a variation of Microsofts BEiT Model\n",
    "## BEiT Model Overview\n",
    "\n",
    "The BEiT model is a Vision Transformer pre-trained in a self-supervised manner on ImageNet-22k and fine-tuned on the same dataset at resolution 224x224. It's capable of image classification tasks and introduced a unique pre-training objective to predict visual tokens from masked patches. The model processes images as sequences of fixed-size patches and employs relative position embeddings for capturing spatial relationships among patches. The model was introduced by Hangbo Bao, Li Dong, and Furu Wei in the paper titled \"BEiT: BERT Pre-Training of Image Transformers\"&#8203;\n",
    "\n",
    "- **Origin**: Introduced by Hangbo Bao, Li Dong, and Furu Wei in the paper titled \"BEiT: BERT Pre-Training of Image Transformers\".\n",
    "- **Pre-training**: Pre-trained on ImageNet-22k dataset in a self-supervised manner.\n",
    "- **Fine-tuning**: Fine-tuned on the same dataset at resolution 224x224.\n",
    "- **Architecture**: Vision Transformer (ViT) that processes images as sequences of fixed-size patches.\n",
    "- **Position Embeddings**: Employs relative position embeddings instead of absolute ones.\n",
    "- **Pre-training Objective**: Unique objective to predict visual tokens from masked patches.\n",
    "- **Classification**: Capable of image classification tasks&#8203;\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a94410-d7b2-4118-8a48-d8919da96c37",
   "metadata": {},
   "source": [
    "### Installing requirements to run huggingface on paperspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c56fcfd",
   "metadata": {},
   "source": [
    "This code cell does the following operations:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701f4021-21fa-4059-8253-d50c6bbcc4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade transformers torch torchvision torchaudio\n",
    "!pip install -q tokenizers==0.14 evaluate\n",
    "!pip install -q bitsandbytes transformers accelerate gradio thread6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec70ebf-b6f3-4b1b-9dda-7826ec7673e5",
   "metadata": {},
   "source": [
    "---\n",
    "### The Model is about 414Megabytes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44511cfd",
   "metadata": {},
   "source": [
    "This code cell does the following operations:\n",
    "\n",
    "- Imports necessary libraries and modules.\n",
    "- Opens and displays the image that was classified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558c3f07-0e24-47f9-a708-a7db45ae07ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "\n",
    "# Define the classifier pipeline\n",
    "classifier = pipeline(model=\"microsoft/beit-base-patch16-224-pt22k-ft22k\")\n",
    "\n",
    "# URL of the image\n",
    "image_url = \"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"\n",
    "\n",
    "# Load and display the image\n",
    "response = requests.get(image_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Turn off axis numbers and ticks\n",
    "plt.show()\n",
    "\n",
    "# Classify the image and print the result\n",
    "result = classifier(image_url)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8bd0e1",
   "metadata": {},
   "source": [
    "\n",
    "## Interpreting the Model's Output\n",
    "\n",
    "When we provide an image to the model, it returns a set of predictions. Each prediction consists of:\n",
    "- **Label**: The category or class the model believes the image belongs to.\n",
    "- **Score**: A confidence score (usually between 0 and 1) indicating how sure the model is about its prediction.\n",
    "\n",
    "The higher the score, the more confident the model is that the image belongs to the corresponding label. In this notebook, we visualize these predictions using a bar chart, where each bar represents a label, and the length of the bar indicates the confidence score.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4005d6e2",
   "metadata": {},
   "source": [
    "\n",
    "This code cell does the following operations:\n",
    "\n",
    "- Creates a visualization using `matplotlib`.\n",
    "- Plots a horizontal bar chart to visualize model predictions.\n",
    "- Opens and displays the image that was classified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad39d274-1b50-444d-b14f-318f3b6221ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract scores and labels from the result\n",
    "scores = [item['score'] for item in result]\n",
    "labels = [item['label'] for item in result]\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create horizontal bar chart\n",
    "bars = ax.barh(labels, scores, color='skyblue')\n",
    "\n",
    "# Invert the y-axis to have the highest score at the top\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Set labels for x and y axis\n",
    "ax.set_xlabel('Prediction Score')\n",
    "ax.set_ylabel('Label')\n",
    "ax.set_title('Model Predictions')\n",
    "\n",
    "# Display the scores next to the bars\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    ax.text(width - 0.03, bar.get_y() + bar.get_height()/2, \n",
    "            f'{width:.2%}', \n",
    "            ha='center', va='center', color='black')\n",
    "\n",
    "# Display the image and the chart\n",
    "img = Image.open(BytesIO(response.content))\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a3686d",
   "metadata": {},
   "source": [
    "---\n",
    "This code cell does the following operations:\n",
    "\n",
    "- Imports necessary libraries and modules.\n",
    "- Creates a visualization using `matplotlib`.\n",
    "- Plots a horizontal bar chart to visualize model predictions.\n",
    "- Opens and displays the image that was classified.\n",
    "___\n",
    "\n",
    "The interactive cell below asks for an image url. \n",
    "- simply go to google images.\n",
    "- search for your desired image\n",
    "- right click the image desired and click \"copy image link\"\n",
    "- paste image link in the \"image URL\" pop up after you run the cell\n",
    "- click the 'Classify and plot' button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097ebce5-f6d3-4b2b-b8bf-56e7e6fc542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from ipywidgets import widgets, HBox, VBox, Layout, Output\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def classify_and_plot(image_url):\n",
    "    \"\"\"\n",
    "    Classifies the image from the given URL using HuggingFace pipeline and plots the results.\n",
    "    \n",
    "    Args:\n",
    "    - image_url (str): URL of the image to be classified.\n",
    "    \"\"\"\n",
    "    # Classify the image and store the result\n",
    "    result = classifier(image_url)\n",
    "\n",
    "    # Extract scores and labels from the result\n",
    "    scores = [item['score'] for item in result]\n",
    "    labels = [item['label'] for item in result]\n",
    "\n",
    "    # Plotting the classified results\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Create horizontal bar chart\n",
    "    bars = ax.barh(labels, scores, color='skyblue')\n",
    "\n",
    "    # Invert the y-axis to have the highest score at the top\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Set labels for x and y axis\n",
    "    ax.set_xlabel('Prediction Score')\n",
    "    ax.set_ylabel('Label')\n",
    "    ax.set_title('Model Predictions')\n",
    "\n",
    "    # Display the scores next to the bars\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax.text(width - 0.03, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.2%}', \n",
    "                ha='center', va='center', color='black')\n",
    "\n",
    "    # Display the image and the chart\n",
    "    response = requests.get(image_url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define the classifier pipeline\n",
    "classifier = pipeline(model=\"microsoft/beit-base-patch16-224-pt22k-ft22k\")\n",
    "\n",
    "# Create an Output widget to capture and display the function's output\n",
    "output_widget = Output()\n",
    "\n",
    "# Create a Text widget for the image URL\n",
    "image_url_widget = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter image URL',\n",
    "    description='Image URL:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Create a Button widget to trigger the classification and plotting\n",
    "classify_button = widgets.Button(\n",
    "    description='Classify and Plot',\n",
    "    button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click to classify and plot',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "# Define the button click event\n",
    "def on_button_click(button):\n",
    "    with output_widget:\n",
    "        clear_output(wait=True)\n",
    "        classify_and_plot(image_url_widget.value)\n",
    "\n",
    "# Bind the button click event to the Button widget\n",
    "classify_button.on_click(on_button_click)\n",
    "\n",
    "# Create a UI with the widgets\n",
    "ui = VBox([image_url_widget, classify_button, output_widget])\n",
    "\n",
    "# Display the UI\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84724aeb",
   "metadata": {},
   "source": [
    "---\n",
    "## Huggingface and Pipelines\n",
    "\n",
    "[Huggingface](https://huggingface.co/) provides a platform for the community to upload and share their trained models. Their ecosystem revolves around the Transformers library, which offers implementations of state-of-the-art deep learning models for various tasks.\n",
    "\n",
    "One of the main features Huggingface offers is the **pipeline**. A pipeline abstracts away the complexities of setting up a model for inference. Instead of worrying about tokenization, model loading, and post-processing, you can simply use a pipeline to get predictions. It's like a high-level helper that streamlines the process from input to output.\n",
    "\n",
    "For instance, the image classification task we're doing in this notebook uses a pipeline. We simply provide an image, and the pipeline handles the rest, giving us the predicted label.\n",
    "\n",
    "### Benefits of Pipelines:\n",
    "1. **Simplicity**: No need to deal with the intricacies of model setup.\n",
    "2. **Versatility**: Pipelines support a wide range of tasks, from text classification to image recognition.\n",
    "3. **Efficiency**: Quickly switch between models and tasks with minimal code changes.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067e08a2",
   "metadata": {},
   "source": [
    "\n",
    "## Exploring the Huggingface Model Hub\n",
    "\n",
    "The [Huggingface Model Hub](https://huggingface.co/models) is a repository of thousands of pre-trained models. You can explore models for various tasks, including image classification, text generation, translation, and more.\n",
    "\n",
    "### Steps to Use a New Model:\n",
    "1. Visit the [Model Hub](https://huggingface.co/models).\n",
    "2. Use the filters to narrow down models suitable for \"Image Classification\".\n",
    "3. Choose a model and take note of its ID (usually in the format `username/model-name`).\n",
    "4. Replace the model ID in the code below with your chosen model's ID.\n",
    "5. Provide an image and get the prediction!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e893f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'your-model-id' with the model ID you chose from the Model Hub\n",
    "model_id = 'your-model-id'\n",
    "\n",
    "# Load the pipeline for your chosen model\n",
    "new_pipeline = pipeline('image-classification', model=model_id)\n",
    "\n",
    "# Classify an image (replace 'path_to_your_image.jpg' with your image path)\n",
    "image_path = 'path_to_your_image.jpg' \n",
    "result = new_pipeline(image_path)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb90e603-939e-4e2a-8730-d89d0b8fb0f7",
   "metadata": {},
   "source": [
    "- To set the \"image_path\" variable, begin by uploading an image through the \"Upload Files\" button located on the left side of the screen.\n",
    "\n",
    "- Following the upload, right-click the image and select the \"Copy Path\" option.\n",
    "- Now, you can paste the copied path in place of 'path_to_your_image.jpg.'\n",
    "- Make sure the path is inside quotes!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
